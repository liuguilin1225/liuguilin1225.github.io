<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Guilin Liu</title>
<style>
  body {
    margin-top: 30px;
    margin-bottom: 30px;
    margin-left: 100px;
    margin-right: 100px;
  }
	img.rounded-img {
		border: 1px solid #eeeeee;
		border-radius: 5px ;
		-moz-border-radius: 5px ;
		-webkit-border-radius: 5px ;
	}  
  h1,body {
    font-size : 15px;
    font-family: Lato, Helvetica, sans-serif;
  }
</style></head>


<body>

<table border="0" cellspacing="20">
  <tbody><tr>
    <td><img class="rounded-img" src="./img/guilin_liu.jpg" style="height:250px"></td>
    <td width="30"></td>
    <td valign="top">
      <font size="6">Guilin Liu</font><br>
      <font size="3"> Research Scientist at NVIDIA<br></font>
      <font size="3"> Santa Clara, CA<br></font>
      <br>
      <font size="3"> <b>Email: </b> firstname+l at company.com<br></font>
      <font size="3"> <a href="https://scholar.google.com/citations?user=zOQj6-gAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp; </font>	    
	<font size="3"> <a href="https://github.com/liuguilin1225">GitHub</a><br></font>
      <font size="3"> <a href="https://www.linkedin.com/in/guilin-liu-1a347456/">LinkedIn</a> &nbsp; </font>	    
	<font size="3"> <a href="https://twitter.com/GuilinL">Twitter</a> &nbsp; <br></font>
	    
    </td>
  </tr></tbody></table>

<font size="3"> 
</font><font size="3"> I am a Senior (Staff level) Research Scientist in at NVIDIA since 2017. My projects at NVIDIA include <a href="https://www.nvidia.com/en-us/geforce/technologies/dlss/"> DLSS </a> (used by millions of gamers), partial conv based inpainting (with over 1.7 million views on <a href="https://www.youtube.com/watch?v=gg0F5JjKmhA">YouTube</a>, featured in tons of media outles and showed as live demo with NVIDIA CEO in GTC keynote), text-to-video used in <a href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">Picasso</a>, large-scale multi-modal models etc. Currently my research interests mainly lie in generative models (diffusion models), large-scale multi-modal models and low-level computer vision. 


<br><br>
<font size="4"> 
<b> I am alway looking for research intern to work on the research topics in multimodal and generative models. Please send me an email with your CV if you are interested. </b>
</font>
<br><br>

    
<!-- </a></font><a> 
 -->

</a></p>

<br>
</a></p><p><a><b>Publications (Please refer to <a href="https://scholar.google.com/citations?hl=en&user=zOQj6-gAAAAJ&view_op=list_works">Google Scholar</a> for the latest ones):</b></a></p><div><a>
<div style="position: relative; left: 10px">
  <table cellpadding="5">

  <tbody>


   <tr>
    <td>
        <img src="https://raw.githubusercontent.com/NVlabs/EAGLE/refs/heads/main/assets/Logo.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</u> <br>
        Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, <b>Guilin Liu</b><br>
        arxiv<br>
      <a href="https://arxiv.org/pdf/2408.15998">Paper</a> &nbsp; <a href="https://github.com/NVlabs/Eagle">Code</a> &nbsp; 
      </td>
    </tr>
	  
    <tr>
      <td>
          <img src="./img/diffit.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
    <u>DiffiT: Diffusion Vision Transformers for Image Generation</u><br>
    Ali Hatamizadeh, Jiaming Song, <b>Guilin Liu</b>, Jan Kautz, Arash Vahdat<br>
       ECCV 2024<br>
    <a href="https://arxiv.org/abs/2312.02139">Paper</a> .
        </td>
    </tr>

    <tr>
      <td>
          <img src="./img/text-to-video.gif" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
    <u>PYOCO: Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models</u><br>
    Songwei Ge, Seungjun Nah, <b>Guilin Liu</b>, Tyler Poon, Andrew Tao, Bryan Catanzaro,  David Jacobs,  Jia-Bin Huang,  Ming-Yu Liu,  Yogesh Balaji<br>
       ICCV 2023<br>
    <a href="https://arxiv.org/abs/2305.10474">Paper</a> &nbsp; <a href="https://research.nvidia.com/labs/dir/pyoco/">Project Page</a>.
        </td>
    </tr>

    <tr>
      <td>
          <img src="./img/partialpadding.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
    <u>Partial Convolution for Padding, Inpainting, and Image Synthesis</u><br>
    <b>Guilin Liu</b>*, Aysegul Dundar*, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan Sapra, Zhiding Yu, Xiaodong Yang, Andrew Tao, Bryan Catanzaro (*equal contribution)<br>
          T-PAMI 2022<br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9903574">Paper</a> &nbsp;
        </td>
    </tr>
  
    <tr>
      <td>
          <img src="./img/couple_segment.PNG" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
          <u>Coupled Segmentation and Edge Learning via Dynamic Graph Propagation</u> <br>
          Zhiding Yu, Rui Huang, Wonmin Byeon, Sifei Liu, <b>Guilin Liu</b>, Thomas Breuel, Anima Anandkumar, Jan Kautz<br>
          NeurIPS 2021<br>
    <a href="https://proceedings.neurips.cc/paper/2021/file/26ddd45b02859e836d13d4b9fde34281-Paper.pdf">Paper</a> &nbsp;
        </td>
    </tr>

    <tr>
      <td>
          <img src="./img/attention_contrast.jpeg" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
          <u>Dual Contrastive Loss and Attention for GANs</u> <br>
          Ning Yu, <b>Guilin Liu</b>, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry Davis, and Mario Fritz<br>
          ICCV 2021<br>
    <a href="https://arxiv.org/pdf/2103.16748.pdf">Paper</a> &nbsp;
        </td>
    </tr>

    <tr>
      <td>
          <img src="./img/discobox.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
          <u>Discobox: Weakly supervised instance segmentation and semantic correspondence from box supervision </u> <br>
          Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan, <b>Guilin Liu</b>, Yuke Zhu, Larry S Davis, Anima Anandkumar<br>
          ICCV 2021<br>
    <a href="https://arxiv.org/abs/2105.06464">Paper</a> <a href="https://github.com/voidrank/DiscoBox">Code</a> &nbsp;
        </td>
      </tr>

   <tr>
    <td>
        <img src="./img/3dinference.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>View Generalization for Single Image Textured 3D Models </u> <br>
        Anand Bhattad, Aysegul Dundar, <b>Guilin Liu</b>, Andrew Tao, Bryan Catanzaro<br>
        CVPR 2021<br>
	<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Bhattad_View_Generalization_for_Single_Image_Textured_3D_Models_CVPR_2021_paper.html">Paper</a> <a href="https://nv-adlr.github.io/view-generalization">Web-page</a> &nbsp;
      </td>
    </tr>

    <tr>
      <td>
          <img src="./img/neural_FFT.jpeg" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
      </td>
      <td>
          <u>Neural FFTs for Universal Texture Image Synthesis</u> <br>
          Morteza Mardani*, <b>Guilin Liu</b>*, Aysegul Dundar, Shiqiu Liu, Andrew Tao, Bryan Catanzaro (*equal contribution)<br>
          NeurIPS 2020<br>
        <a href="https://papers.nips.cc/paper/2020/file/a23156abfd4a114c35b930b836064e8b-Paper.pdf">Paper</a>
        </td>
      </tr>	   
     <tr>

    <tr>
    <td>
        <img src="./img/front_page.PNG" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter</u> <br>
         <b>Guilin Liu</b>, Rohan Taori, Ting-Chun Wang, Zhiding Yu, Shiqiu Liu, Fitsum A. Reda, Karan Sapra, Andrew Tao, Bryan Catanzaro<br>
        arxiv preprint<br>
      <a href="https://arxiv.org/pdf/2007.07243.pdf">Paper</a>  &nbsp; <a href="https://www.youtube.com/watch?v=ej1NDiMT99g">1 min video</a> &nbsp; <a href="https://www.youtube.com/watch?v=8Us9c5iBRCY">6 min video</a>
      </td>
    </tr>	  
	  
   <tr>
    <td>
        <img src="./img/panoptic_img_synthesis.PNG" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Panoptic-based Image Synthesis</u> <br>
        Aysegul Dundar, Karan Sapra, <b>Guilin Liu</b>, Andrew Tao, Bryan Catanzaro<br>
        CVPR 2020<br>
      <a href="https://arxiv.org/pdf/2004.10289.pdf">Paper</a>
      </td>
    </tr>	   
	 
	  
   <tr>
    <td>
        <img src="./img/partialpadding.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Partial Convolution based Padding</u> <br>
        <b>Guilin Liu</b>, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan Sapra, Zhiding Yu, Xiaodong Yang, Andrew Tao, Bryan Catanzaro<br>
        arXiv preprint<br>
      <a href="https://arxiv.org/pdf/1811.11718.pdf">Paper</a> &nbsp; <a href="https://github.com/NVIDIA/partialconv">Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/dance.gif" class="rounded-img" style="width:180px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Few-Shot Video-to-Video Synthesis</u> <br>
        Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, <b>Guilin Liu</b>, Jan Kautz, Bryan Catanzaro<br>
        NeurIPS 2019<br>
       <a href="https://nvlabs.github.io/few-shot-vid2vid/">Project</a> &nbsp; <a href="https://arxiv.org/abs/1910.12713">Paper</a> &nbsp; <a href="https://nvlabs.github.io/few-shot-vid2vid/">Github</a> &nbsp;  <a href="https://www.youtube.com/watch?v=8AZBuyEuDqc&feature=youtu.be">Youtube</a>
      </td>
    </tr>	  
	  
   <tr>
    <td>
        <img src="./img/unsupervise_video.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Unsupervised Video Interpolation Using Cycle Consistency</u> <br>
        Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, <b>Guilin Liu</b>, Kevin J Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro<br>
        ICCV 2019<br>
        <a href="https://arxiv.org/pdf/1906.05928.pdf">Paper</a> &nbsp; 
      </td>
    </tr>
	  
	  
   <tr>
    <td>
        <img src="./img/arxiv2019_scenes.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Neural Inverse Rendering of an Indoor Scene from a Single Image</u> <br>
        Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, <b>Guilin Liu</b>, David Jacobs, Jan Kautz<br>
        ICCV 2019<br>
      <a href="https://arxiv.org/abs/1901.02453">Paper</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/eccv2018_inpainting.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Image Inpainting for Irregular Holes Using Partial Convolutions</u> <br>
        <b>Guilin Liu</b>, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro<br>
        ECCV 2018<br>
      <a href="https://arxiv.org/pdf/1804.07723.pdf">Paper</a> &nbsp; <a href="https://nv-adlr.github.io/publication/partialconv-inpainting">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=gg0F5JjKmhA">Video</a> &nbsp;  <a href="http://fortune.com/2018/04/24/nvidia-artificial-intelligence-images/">Fortune</a> &nbsp; <a href="https://www.forbes.com/sites/nvidia/2018/06/15/ai-research-is-pushing-the-limits-of-whats-possible/?linkId=100000002773533#60e963d5ae75">Forbes</a> &nbsp; <a href="https://youtu.be/cgG3h87IeIo?t=2938"> GTC Keynote Live Demo with NVIDIA CEO Jensen Huang </a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/vid2vid.gif" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Video-to-Video Synthesis</u> <br>
        Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, <b>Guilin Liu</b>, Andrew Tao, Jan Kautz, Bryan Catanzaro<br>
        NeurIPS 2018<br>
      <a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf">Paper</a> &nbsp; <a href="https://tcwang0509.github.io/vid2vid/">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=S1OwOd-war8&feature=youtu.be">Video</a> &nbsp; <a href="https://arxiv.org/abs/1808.06601">Arxiv</a> &nbsp; <a href="https://github.com/NVIDIA/vid2vid"> Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/sdcnet.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>SDC-Net: Video prediction using spatially-displaced convolution</u> <br>
        Fitsum A. Reda, <b>Guilin Liu</b>, Kevin J. Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao, Bryan Catanzaro<br>
        ECCV 2018<br>
        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fitsum_Reda_SDC-Net_Video_prediction_ECCV_2018_paper.pdf">Paper</a> &nbsp; 
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/iccv2017_material.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Material Editing Using a Physically Based Rendering Network</u> <br>
        <b>Guilin Liu</b>, Duygu Ceylan, Ersin Yumer, Jimei Yang, Jyh-Ming Lien<br>
        ICCV 2017<br>
      <a href="https://arxiv.org/pdf/1708.00106.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/material">Project</a> &nbsp; <a href="https://www.dropbox.com/s/atycd58bnp37gck/Car_21.zip?dl=0">Data</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/arxiv2016_symmetry.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Symmetry-aware Depth Estimation using Deep Neural Networks</u> <br>
        <b>Guilin Liu</b>, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang<br>
        arxiv 2016<br>
      <a href="https://arxiv.org/pdf/1604.06079.pdf">Arxiv</a> &nbsp;
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/spm2016_decompose.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Nearly Convex Segmentation of Polyhedra Through Convex Ridge Separation</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        SPM 2016, also in Journal of Computer-Aided Design<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/GuilinLiu/CAD2016_Shape_Segmentation.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/CoRise">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=QR5bSpkSjPU">Video</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/cvpr2015_cvf.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Continuous Visibility Feature</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        CVPR 2015<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/CVF/cvpr15-cvf.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/CVF">Project</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Software#cvf">Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/iros2015_svma.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Fast Medial Axis Approximation via Max-Margin Pushing</u> <br>
        <b>Guilin Liu</b>, Jyh-Ming Lien<br>
        IROS 2015<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/GuilinLiu/svm-ma-sampling.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/SVMA">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=5372aVzw4Cs">Video</a>
      </td>
    </tr>

	 <tr>
    <td>
        <img src="./img/cvpr14_dude_2.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
	  <td>
        <u>Dual-Space Decomposition of 2D Complex Shapes</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        CVPR 2014<br>
		  <a href="http://masc.cs.gmu.edu/wiki/uploads/Dude2D/dude2d.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Dude2D">Project</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Software#dude2d">Code</a>
      </td>
    </tr>
	
	
  </tbody>
  </table>
</div>
</a>



</a></p><p><a><b>Collaboration Interns</b></a></p><div><a>
  <div>
    2022 <a href="https://www.linkedin.com/in/tyler-poon-138592149/">Tyler Poon</a>. University of Chicago <br>
    2022 <a href="https://songweige.github.io/">Songwei Ge</a>. University of Maryland <br>
    2020 <a href="https://anandbhattad.github.io/">Anand Bhattad</a>. University of Illinois Urbana Champaign. Now Research Assistant Professor at Toyota Technological Institute at Chicago. <br>
    2020 <a href="https://ningyu1991.github.io/">Ning Yu</a>. University of Maryland. Now Research Scientist at Salesforce Research. <br>
    2020 <a href="https://voidrank.github.io/">Shiyi Lan</a>. University of Maryland. Now Research Scientist at NVIDIA. <br>
    2019 <a href="https://www.rohantaori.com/">Rohan Taori</a>. UC Berkeley. Now PhD student at Stanford University. <br>
    2018 <a href="https://www.cs.unc.edu/~ronisen/">Soumyadip Sengupta</a>. University of Maryland. Now Assistant Professor at University of North Carolina at Chapel Hill. <br>
  </div>

</a></p><p><a><b>Co-organized Workshop & Tutorial</b></a></p><div><a>
<div>
<a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/">ICCV 2019 Tutorial on Accelerating Computer Vision with Mixed Precision</a>
</div>
	
	
<p><a><b>Media Coverage:</b></a></p>
<div>
<a href="http://fortune.com/2018/04/24/nvidia-artificial-intelligence-images/">Fortune</a>, <a href="https://www.forbes.com/sites/nvidia/2018/06/15/ai-research-is-pushing-the-limits-of-whats-possible/?linkId=100000002773533#54ef3a64ae75">Forbes</a>, <a href="https://www.fastcompany.com/40563129/ai-can-now-reconstruct-your-exs-scratched-out-face-in-photos">Fast Company</a>, <a href="https://www.engadget.com/2018/04/24/nvidia-ai-fixes-photos/">Engadget</a>, <a href="https://www.slashgear.com/nvidia-neural-network-reconstructs-images-with-missing-parts-25528443/">SlashGear</a>, <a href="https://www.digitaltrends.com/photography/nvidia-inpainting-ai-healing-brush-tool/">Digital Trends</a>, <a href="https://thenextweb.com/artificial-intelligence/2018/04/24/nvidias-ai-reconstructs-partially-erased-images-with-jaw-dropping-accuracy/">TNW</a>, <a href="https://www.eteknix.com/nvidia-shows-off-impressive-ai-photo-reconstruction-abilities/">eTeknix</a>, <a href="http://www.game-debate.com/news/24974/nvidias-new-ai-tech-can-reconstruct-corrupt-images-with-near-perfect-results">Game Debate</a>, <a href="http://www.alphr.com/artificial-intelligence/1009180/nvidia-ai-restore-damaged-old-photos">Alphr</a>, <a href="https://www.gizbot.com/news/nvidia-s-new-ai-imaging-technique-can-resurrect-your-old-damaged-pictures-050009.html">Gizbot</a>, <a href="https://fossbytes.com/nvidia-imaging-technique-reconstruct-photos/">Fossbytes</a>, <a href="https://www.techradar.com/news/nvidias-amazing-deep-learning-tool-can-reconstruct-incomplete-photos">Techradar</a>, <a href="https://beebom.com/nvidia-feature-repair-images/">Beeborn</a>, <a href="https://www.techradar.com/news/nvidias-amazing-deep-learning-tool-can-reconstruct-incomplete-photos">Bit-tech</a>, <a href="http://www.hexus.net/tech/news/software/117515-nvidia-shows-ai-tech-realistic-reconstruction-photos/">Hexus</a>, <a href="https://hothardware.com/news/nvidia-inpainting-ai-rebuild-corrupted-damaged-images">HotHardWare</a>, <a href="https://www.bleepingcomputer.com/news/technology/nvidia-develops-ai-that-reconstructs-corrupted-images/">BleepingComputer</a>, <a href="https://www.hardocp.com/news/2018/04/23/nvidia_ai_inpainting_cool_as_hell">hardocp</a>, <a href="https://boingboing.net/2018/04/23/a-i-reconstructs-incomplete-p.html">boingboing</a>, <a href="https://petapixel.com/2018/04/23/nvidias-ai-powered-content-aware-fill-is-mind-blowing/">PetaPixel</a>, <a href="http://www.sohu.com/a/229392616_473283">&#25628;&#29392;</a>, <a href="http://t.cj.sina.com.cn/articles/view/1649597805/6252dd6d019005h9n">&#26032;&#28010;</a>, <a href="https://zhuanlan.zhihu.com/p/36116821">&#37327;&#23376;&#20301;(&#30693;&#20046;)</a>
</div>

<br><br><br><br>

</body>
</html>
