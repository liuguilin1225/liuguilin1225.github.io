<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Guilin Liu</title>
<style>
  body {
    margin-top: 30px;
    margin-bottom: 30px;
    margin-left: 100px;
    margin-right: 100px;
  }
	img.rounded-img {
		border: 1px solid #eeeeee;
		border-radius: 5px ;
		-moz-border-radius: 5px ;
		-webkit-border-radius: 5px ;
	}  
  h1,body {
    font-size : 15px;
    font-family: Lato, Helvetica, sans-serif;
  }
</style></head>


<body>

<table border="0" cellspacing="20">
  <tbody><tr>
    <td><img class="rounded-img" src="./img/Guilin_Liu_portrait.jpg" style="height:250px"></td>
    <td width="30"></td>
    <td valign="top">
      <font size="6">Guilin Liu</font><br>
      <font size="3"> Senior Research Scientist at NVIDIA<br></font>
      <font size="3"> Santa Clara, CA<br></font>
      <br>
      <font size="3"> <b>Email: </b> guilinl at nvidiaaaaaaa.com<br></font>
      <font size="3"> <a href="https://scholar.google.com/citations?user=zOQj6-gAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp; </font>	    
	<font size="3"> <a href="https://github.com/liuguilin1225">GitHub</a><br></font>
      <font size="3"> <a href="https://www.linkedin.com/in/guilin-liu-1a347456/">LinkedIn</a> &nbsp; </font>	    
	<font size="3"> <a href="https://twitter.com/GuilinL">Twitter</a> &nbsp; <br></font>
	    
    </td>
  </tr></tbody></table>

<font size="3"> 
</font><font size="3"> I am a Senior Research Scientist in Applied Deep Learning Research group at NVIDIA, where we do deep learning related research. I got my Ph.D. in Computer Science from George Mason University in 2017 summer. In 2012, I got a Bachelor's degree in Spatial Informatics & Digitalized Technology(Software Engineering and Georgraphy Information System) and a minor degree in Finance from Wuhan University. I was a research intern at TTI Chicago in 2015 summer and at Adobe Research in 2016 summer.

<!-- <br><br>
<font size="4"> 
<b> I am looking for research intern to work on the research topics in deep learning for vision and graphics. Please send me an email with your CV if you are interested. </b>
</font>
<br><br>
-->

    
<!-- </a></font><a> 
 -->

</a></p><p><a><b>News:</b></a></p><div><a>
Jan. 2020: Our research on DLSS 2.0 has helped real-time graphics to achieve both better image quality and higher frame rate. It is (one of) the first projects to successfully leverage the power of DL onto real-time graphics. It has been deployed in many games including: Death Stranding, Battlefield V, Monster Hunter: World, Control, Deliver Us The Moon, Bright Memory, Wolfenstein Youngblood, Mechwarrior V: Mercenaries, Final Fantasy XV, Anthem, Shadow of the Tomb Raider, Metro Exodus etc <a href="https://www.youtube.com/watch?v=gpzFX4P1Jow&feature=emb_title">Video</a>, <a href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/">Report</a>, <a href="http://behindthepixels.io/assets/files/DLSS2.0.pdf">Slides</a>. <br>


Oct. 2019: We organized <a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/"> Turtorial on Accelerating Computer Vision with Mixed Precision </a> at ICCV 2019. <br>
Nov. 2018: Code and Paper of Partial Convolution based Padding (better than all existing padding schemes) is released at <a href="https://github.com/NVIDIA/partialconv">Code</a>. <br>
Sep. 2018: Our inpainting online demo is now available at <a href="https://www.nvidia.com/research/inpainting/">https://www.nvidia.com/research/inpainting/</a> (Note: the natural image model is the consistent with model describled in ECCV paper; the face image model has been further improved by using GAN loss to train the same network after ECCV. We also suggest to do continuous inpainting, uploading the inpainting results to do second-time inpainting, to get better results.) <br>
Sep. 2018: <a href="https://github.com/NVIDIA/vid2vid">Video-to-Video Synthesis</a> was accepted to NIPS 2018. <br>	
July 2018: Two papers got accepted to ECCV 2018. <br>	
May 2018: Showed image inpainting demo during NVIDIA CEO Jensen Huang's keynote talk at <a href="https://youtu.be/cgG3h87IeIo?t=2938"> GTC Taiwan </a>. <br>	
May 2018: Recently we released a new paper <a href="http://masc.cs.gmu.edu/wiki/partialconv"> Image Inpainting for Irregular Holes Using Partial Convolutions (project page with FAQ) </a>. The Youtube video can be found <a href="https://www.youtube.com/watch?v=gg0F5JjKmhA">here </a>, which has been viewed over 1,000,000 times. This project was also featured in many presses, including <a href="http://fortune.com/2018/04/24/nvidia-artificial-intelligence-images/"> Fortune</a>, <a href="https://www.forbes.com/sites/nvidia/2018/06/15/ai-research-is-pushing-the-limits-of-whats-possible/?linkId=100000002773533#4b351f01ae75"> Forbes</a>.
</div>

<br>
</a></p><p><a><b>Publications:</b></a></p><div><a>
<div style="position: relative; left: 10px">
  <table cellpadding="5">

  <tbody>

    <tr>
    <td>
        <img src="./img/front_page.PNG" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter</u> <br>
         <b>Guilin Liu</b>, Rohan Taori, Ting-Chun Wang, Zhiding Yu, Shiqiu Liu, Fitsum A. Reda, Karan Sapra, Andrew Tao, Bryan Catanzaro<br>
        arxiv preprint<br>
      <a href="https://arxiv.org/pdf/2007.07243.pdf">Paper</a>  &nbsp; <a href="https://www.youtube.com/watch?v=ej1NDiMT99g">1 min video</a> &nbsp; <a href="https://www.youtube.com/watch?v=8Us9c5iBRCY">6 min video</a>
      </td>
    </tr>	  
	  
   <tr>
    <td>
        <img src="./img/panoptic_img_synthesis.PNG" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Panoptic-based Image Synthesis</u> <br>
        Aysegul Dundar, Karan Sapra, <b>Guilin Liu</b>, Andrew Tao, Bryan Catanzaro<br>
        CVPR 2020<br>
      <a href="https://arxiv.org/pdf/2004.10289.pdf">Paper</a>
      </td>
    </tr>	   
	 
	  
   <tr>
    <td>
        <img src="./img/partialpadding.png" class="rounded-img" style="width:160px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Partial Convolution based Padding</u> <br>
        <b>Guilin Liu</b>, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan Sapra, Zhiding Yu, Xiaodong Yang, Andrew Tao, Bryan Catanzaro<br>
        arXiv preprint<br>
      <a href="https://arxiv.org/pdf/1811.11718.pdf">Paper</a> &nbsp; <a href="https://github.com/NVIDIA/partialconv">Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/dance.gif" class="rounded-img" style="width:180px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Few-Shot Video-to-Video Synthesis</u> <br>
        Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, <b>Guilin Liu</b>, Jan Kautz, Bryan Catanzaro<br>
        NeurIPS 2019<br>
       <a href="https://nvlabs.github.io/few-shot-vid2vid/">Project</a> &nbsp; <a href="https://arxiv.org/abs/1910.12713">Paper</a> &nbsp; <a href="https://nvlabs.github.io/few-shot-vid2vid/">Github</a> &nbsp;  <a href="https://www.youtube.com/watch?v=8AZBuyEuDqc&feature=youtu.be">Youtube</a>
      </td>
    </tr>	  
	  
   <tr>
    <td>
        <img src="./img/unsupervise_video.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Unsupervised Video Interpolation Using Cycle Consistency</u> <br>
        Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, <b>Guilin Liu</b>, Kevin J Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro<br>
        ICCV 2019<br>
        <a href="https://arxiv.org/pdf/1906.05928.pdf">Paper</a> &nbsp; 
      </td>
    </tr>
	  
	  
   <tr>
    <td>
        <img src="./img/arxiv2019_scenes.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Neural Inverse Rendering of an Indoor Scene from a Single Image</u> <br>
        Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, <b>Guilin Liu</b>, David Jacobs, Jan Kautz<br>
        ICCV 2019<br>
      <a href="https://arxiv.org/abs/1901.02453">Paper</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/eccv2018_inpainting.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Image Inpainting for Irregular Holes Using Partial Convolutions</u> <br>
        <b>Guilin Liu</b>, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro<br>
        ECCV 2018<br>
      <a href="https://arxiv.org/pdf/1804.07723.pdf">Paper</a> &nbsp; <a href="https://nv-adlr.github.io/publication/partialconv-inpainting">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=gg0F5JjKmhA">Video</a> &nbsp;  <a href="http://fortune.com/2018/04/24/nvidia-artificial-intelligence-images/">Fortune</a> &nbsp; <a href="https://www.forbes.com/sites/nvidia/2018/06/15/ai-research-is-pushing-the-limits-of-whats-possible/?linkId=100000002773533#60e963d5ae75">Forbes</a> &nbsp; <a href="https://youtu.be/cgG3h87IeIo?t=2938"> GTC Keynote Live Demo with NVIDIA CEO Jensen Huang </a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/vid2vid.gif" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Video-to-Video Synthesis</u> <br>
        Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, <b>Guilin Liu</b>, Andrew Tao, Jan Kautz, Bryan Catanzaro<br>
        NeurIPS 2018<br>
      <a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf">Paper</a> &nbsp; <a href="https://tcwang0509.github.io/vid2vid/">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=S1OwOd-war8&feature=youtu.be">Video</a> &nbsp; <a href="https://arxiv.org/abs/1808.06601">Arxiv</a> &nbsp; <a href="https://github.com/NVIDIA/vid2vid"> Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/sdcnet.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>SDC-Net: Video prediction using spatially-displaced convolution</u> <br>
        Fitsum A. Reda, <b>Guilin Liu</b>, Kevin J. Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao, Bryan Catanzaro<br>
        ECCV 2018<br>
        <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fitsum_Reda_SDC-Net_Video_prediction_ECCV_2018_paper.pdf">Paper</a> &nbsp; 
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/iccv2017_material.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Material Editing Using a Physically Based Rendering Network</u> <br>
        <b>Guilin Liu</b>, Duygu Ceylan, Ersin Yumer, Jimei Yang, Jyh-Ming Lien<br>
        ICCV 2017<br>
      <a href="https://arxiv.org/pdf/1708.00106.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/material">Project</a> &nbsp; <a href="https://www.dropbox.com/s/atycd58bnp37gck/Car_21.zip?dl=0">Data</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/arxiv2016_symmetry.JPG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Symmetry-aware Depth Estimation using Deep Neural Networks</u> <br>
        <b>Guilin Liu</b>, Chao Yang, Zimo Li, Duygu Ceylan, Qixing Huang<br>
        arxiv 2016<br>
      <a href="https://arxiv.org/pdf/1604.06079.pdf">Arxiv</a> &nbsp;
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/spm2016_decompose.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Nearly Convex Segmentation of Polyhedra Through Convex Ridge Separation</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        SPM 2016, also in Journal of Computer-Aided Design<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/GuilinLiu/CAD2016_Shape_Segmentation.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/CoRise">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=QR5bSpkSjPU">Video</a>
      </td>
    </tr>


   <tr>
    <td>
        <img src="./img/cvpr2015_cvf.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Continuous Visibility Feature</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        CVPR 2015<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/CVF/cvpr15-cvf.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/CVF">Project</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Software#cvf">Code</a>
      </td>
    </tr>

   <tr>
    <td>
        <img src="./img/iros2015_svma.PNG" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
    <td>
        <u>Fast Medial Axis Approximation via Max-Margin Pushing</u> <br>
        <b>Guilin Liu</b>, Jyh-Ming Lien<br>
        IROS 2015<br>
      <a href="http://masc.cs.gmu.edu/wiki/uploads/GuilinLiu/svm-ma-sampling.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/SVMA">Project</a> &nbsp; <a href="https://www.youtube.com/watch?v=5372aVzw4Cs">Video</a>
      </td>
    </tr>

	 <tr>
    <td>
        <img src="./img/cvpr14_dude_2.png" class="rounded-img" style="width:140px">&nbsp;&nbsp; 
    </td>
	  <td>
        <u>Dual-Space Decomposition of 2D Complex Shapes</u> <br>
        <b>Guilin Liu</b>, Zhonghua Xi, Jyh-Ming Lien<br>
        CVPR 2014<br>
		  <a href="http://masc.cs.gmu.edu/wiki/uploads/Dude2D/dude2d.pdf">Paper</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Dude2D">Project</a> &nbsp; <a href="http://masc.cs.gmu.edu/wiki/Software#dude2d">Code</a>
      </td>
    </tr>
	
	
  </tbody>
  </table>
</div>
</a>



</a></p><p><a><b>Co-organized Workshop & Tutorial</b></a></p><div><a>
<div>
<a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/">ICCV 2019 Tutorial on Accelerating Computer Vision with Mixed Precision</a>
</div>
	
	
<p><a><b>Media Coverage:</b></a></p>
<div>
<a href="http://fortune.com/2018/04/24/nvidia-artificial-intelligence-images/">Fortune</a>, <a href="https://www.forbes.com/sites/nvidia/2018/06/15/ai-research-is-pushing-the-limits-of-whats-possible/?linkId=100000002773533#54ef3a64ae75">Forbes</a>, <a href="https://www.fastcompany.com/40563129/ai-can-now-reconstruct-your-exs-scratched-out-face-in-photos">Fast Company</a>, <a href="https://www.engadget.com/2018/04/24/nvidia-ai-fixes-photos/">Engadget</a>, <a href="https://www.slashgear.com/nvidia-neural-network-reconstructs-images-with-missing-parts-25528443/">SlashGear</a>, <a href="https://www.digitaltrends.com/photography/nvidia-inpainting-ai-healing-brush-tool/">Digital Trends</a>, <a href="https://thenextweb.com/artificial-intelligence/2018/04/24/nvidias-ai-reconstructs-partially-erased-images-with-jaw-dropping-accuracy/">TNW</a>, <a href="https://www.eteknix.com/nvidia-shows-off-impressive-ai-photo-reconstruction-abilities/">eTeknix</a>, <a href="http://www.game-debate.com/news/24974/nvidias-new-ai-tech-can-reconstruct-corrupt-images-with-near-perfect-results">Game Debate</a>, <a href="http://www.alphr.com/artificial-intelligence/1009180/nvidia-ai-restore-damaged-old-photos">Alphr</a>, <a href="https://www.gizbot.com/news/nvidia-s-new-ai-imaging-technique-can-resurrect-your-old-damaged-pictures-050009.html">Gizbot</a>, <a href="https://fossbytes.com/nvidia-imaging-technique-reconstruct-photos/">Fossbytes</a>, <a href="https://www.techradar.com/news/nvidias-amazing-deep-learning-tool-can-reconstruct-incomplete-photos">Techradar</a>, <a href="https://beebom.com/nvidia-feature-repair-images/">Beeborn</a>, <a href="https://www.techradar.com/news/nvidias-amazing-deep-learning-tool-can-reconstruct-incomplete-photos">Bit-tech</a>, <a href="http://www.hexus.net/tech/news/software/117515-nvidia-shows-ai-tech-realistic-reconstruction-photos/">Hexus</a>, <a href="https://hothardware.com/news/nvidia-inpainting-ai-rebuild-corrupted-damaged-images">HotHardWare</a>, <a href="https://www.bleepingcomputer.com/news/technology/nvidia-develops-ai-that-reconstructs-corrupted-images/">BleepingComputer</a>, <a href="https://www.hardocp.com/news/2018/04/23/nvidia_ai_inpainting_cool_as_hell">hardocp</a>, <a href="https://boingboing.net/2018/04/23/a-i-reconstructs-incomplete-p.html">boingboing</a>, <a href="https://petapixel.com/2018/04/23/nvidias-ai-powered-content-aware-fill-is-mind-blowing/">PetaPixel</a>, <a href="http://www.sohu.com/a/229392616_473283">&#25628;&#29392;</a>, <a href="http://t.cj.sina.com.cn/articles/view/1649597805/6252dd6d019005h9n">&#26032;&#28010;</a>, <a href="https://zhuanlan.zhihu.com/p/36116821">&#37327;&#23376;&#20301;(&#30693;&#20046;)</a>
</div>

<br><br><br><br>

</body>
</html>
